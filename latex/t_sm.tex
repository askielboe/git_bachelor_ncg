\section{Theory}
\subsection{Quantum field theory}
Before we can get started with noncommutative geometry, we will need a quick overview of the Standard Model. The particles and interactions of the SM are described in the language of quantum mechanics and quantum field theory. To understand some of the problems related to quantizing the gravitational field, we will quickly describe some basic principles of classical mechanics and QFT. Working from a bottom-up approach, we start out with a very short introduction to the action principle and Feynman's path integral.

\subsubsection{Principle of least action}
The action $\mathcal{S}$ is a functional\footnote{A functional is something that takes as its argument a function and returns a scalar.} related to the time evolution of a physical system. Namely, the action is the change in phase of the wave function of the system. The Lagrangian is defined as the rate of change of the phase and given as 

\begin{equation}\label{eq:S}
	\mathcal{S} = \int_{t_i}^{t_f} L(x,\dot{x},t)dt,
\end{equation}

where $x, \dot{x} \equiv \frac{dx}{dt}$ are generalized coordinates and we integrate over all intermediate states taken by the system, evolving from time $t_i$ to $t_f$. The principle of least action states that the actual path taken by the system is one that minimizes the action, or more precisely one where the action is stationary. In other words the variation of the line integral going from $t_i$ to $t_f$ is zero \cite{goldstein1959};

\begin{equation}
	\delta \mathcal{S} = \delta \int_{t_i}^{t_f} L \textrm{d}t = 0.
\end{equation}

What this somewhat mysterious principle states is actually something that is ordinarily quite intuitive; namely that if you drop a ball at the top of a hill, the ball will roll down the hill covering the shortest possible distance. You could say that the ball chooses the easiest path, i.e. one that minimizes the action of the ball. Although this rather seems obvious, there is actually no particular reason that this principle should hold true, but rather is only something we can infer, empirically, from experience. Nonetheless it has proven to be an immensely powerful principle in physics. Using Hamilton's principle, and knowing the Lagrangian of the system, one can derive the complete equations of motion of the system. This was how it was done in classical mechanics and it is this same principle we use in quantum mechanics to obtain the time evolution of a physical state.

\subsubsection{The path integral formalism} \label{sec:feynpath}
In 1948 Richard Feynman presented his new formulation of non-relativistic quantum mechanics, using what is known as the path integral formalism \cite{feynman1948sta}. The path integral takes into account all possible phase space paths that a particle, or any other quantum mechanical system going from a state $\ket{i}$ to a state $\ket{f}$, can take. This can be changes in position, momentum, energy, or other intrinsic variables, and is represented as an intermediate state of the system. Generally, and this is actually a central point in quantum mechanics, we are not interested in the path taken, but only in the fact that at some time $t_{i}$ the particle was in a state $\ket{i}$ and at some later, time $t_{f}$, the particle is in a state $\ket{f}$. In order to arrive at this quantum probability-amplitude we must sum, or integrate, over the infinitely many paths, that the system can take to go from initial to final state. Thus, the amplitude is given as \cite{pathinqft}
% \cite{richter_path_integrals}
\begin{equation} \label{eq:path}
	\bra{f} U(t_f,t_i) \ket{i} = \mathcal{N} \int \mathcal{D}x e^{i\mathcal{S}},
\end{equation}
where $\mathcal{N}$ is a constant of the system, $U$ is the time translation operator, $\mathcal{D}$ is an abbreviation given by the relation 
\begin{equation}
	\int \mathcal{D} x \equiv \lim_{N \to \infty} \int dx_1 \dots dx_N,
\end{equation}

and the action, $\mathcal{S}$, is related to the Lagrangian by the action principle, given for the 1-D case in equation \ref{eq:S}. Equation \eqref{eq:path} is called the propagator in QFT, it describes the probability amplitude that a particle will go from the initial state to the final state in time $t_f - t_i$. So although the particle, in analog to classical mechanics, will most likely to go along the path that minimizes the action, we now allow for the particle to choose a different path, albeit with a lower probability. We can never decide which path the particle actually took, but experiments reveal that Feynman's path integral make incredibly accurate predictions.

\subsubsection{Symmetry groups and gauge theories}
Central to the SM are types of theories, called gauge theories. They are closely related to symmetry groups and operations which leaves the physical system unchanged. Noether's theorem \cite{noether1971ivp} states that for any transformation, under which a physical system is left invariant, there exists a conserved quantity of that system. The classical examples are translations and rotations in the three spatial dimensions. If the system is invariant under these two transformations, it corresponds to the conservation of linear and angular momentum. The transformations form a symmetry group of the theory. The two physically equivalent situations can then be described by different mathematical configurations depending on the reference frame. These configurations are related to each other by the symmetry group. This is the essence of the gauge principle.

In physics, any field theory in which the Lagrangian is invariant under some kind of continuous transformation, is called a gauge theory. The invariance referred to as a gauge invariance. The group of transformations under which the Lagrangian is invariant is called the gauge group of the theory.\footnote{For a historical account of the development of gauge theories, at a level relevant for the present discussion, see \cite{gross1992gtp}.}

\subsubsection{Quantum electrodynamics and SU(1)}
In quantum mechanics, the wave equation is invariant under local, space-time dependent, phase transformations, corresponding to rotations in the complex phase space of the wave function. In practice, this invariance comes about when, in order to make predictions of physical observable, we have to take the absolute square of the wave function. In doing so, information about the phase of the complex wave function is lost and we are left with only the absolute size of the complex number. This means, that we can never physically measure the phase of the wave function, only its amplitude. If we want the wave functions to represent actual physical systems, we have to insist that our equations be invariant under all local phase transformations\footnote{Actually the argument is a bit more subtle. There is no physical principle that states that the wavefunction \emph{has} to be invariant under local phase transformations, but if we want the physics to be the same in our office downstairs and JÃ¸rgens office upstairs, we will have to insist on local gauge invariance.} of $\psi$ given by
\begin{equation} \label{eq:localphase}
    \psi \rightarrow e^{i\alpha(\mathbf{x})} \psi.
\end{equation}
Together, these transformations form the mathematical symmetry group SU(1); the group of all transformations of the form \eqref{eq:localphase}.  So far, this does not seem to pose any problems for us. Adding a phase may change the wave function, but all predictions made about the position of the particle, described by this function, are left invariant. Specifically
\begin{equation}
	|\psi|^2 \rightarrow |e^{i\alpha(\mathbf{x})} \psi|^2 = |\psi|^2.
\end{equation}
However, for the derivative operator $\partial_\mu$ to be well-defined for all points in space-time, we have to replace it by the covariant derivative
\begin{equation} \label{eq:covariant}
	\partial_\mu \rightarrow D_\mu = \partial_\mu - \textrm{i}e A_\mu.
\end{equation}

The covariant derivative compensates for the fact that, when making the local phase change, we pick up an extra factor of $i (\partial_\mu \theta)$ in the derivative of the Lagrangian. So we have introduced a new vector field $A_\mu$, designed to exactly cancel this extra factor, making sure, that gauge invariance is satisfied under local continuous SU(1) transformations. The field, it turns out, is actually just the electromagnetic field known from classical theory of electromagnetism. When $A_\mu$ is quantized in the context of quantum field theory, the quanta of this field are the photons, known as the gauge bosons of QED. By insistence on local gauge invariance of the Lagrangian of the free electron, we get the electromagnetic field, and corresponding interactions, almost for free. This principle, is the driving force behind the success of gauge theories.

\subsubsection{Gauging the SU(2) and SU(3) symmetry groups} \label{sec:su2andsu3}
The previous discussion outlines many of the essential principles of gauge theories. Following, the same logic, the gauge group governing weak interaction, turns out to be the composed group SU(1) $\times$ SU(2), where SU(2) is the group of $2 \times 2$ complex unitary matrices.\footnote{More specifically SU(2) is a Lie group and its Lie algebra is the set of anti-Hermition $2 \times 2$ matrices with trace 0. For more information about Lie groups and Lie algebras see  \cite{fulton1991rtf}.} The electromagnet and weak interactions are then combined into one theory, the electroweak theory. This theory of electroweak interactions was first formulated in 1960s by physicists S. Weinberg and A. Salam, for which they won the 1979 Nobel Price. The generators of SU(2) are the Pauli matrices
\begin{equation}
	\sigma_1 =
	\begin{pmatrix}
	0&1\\
	1&0
	\end{pmatrix}, \quad
	\sigma_2 = 
	\begin{pmatrix}
	0&-i\\
	i&0
	\end{pmatrix}, \quad
	\sigma_3 = 
	\begin{pmatrix}
	1&0\\
	0&-1
	\end{pmatrix}.
\end{equation}
The gauge bosons associated with the weak force are the $W^+$, $W^-$ and the $Z^0$ bosons. Turning our attention to the strong interaction, the gauge bosons are characterized by the SU(3) gauge group. The generators of SU(3) are the 8 Gell-Mann matrices
\begin{align} \label{eq:gellmannmatrices}
	\lambda_1 &= \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix},
	\lambda_2 = \begin{pmatrix} 0 & -i & 0 \\ i & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix},
	\lambda_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 0 \end{pmatrix},
	\lambda_4 = \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix}, \nonumber \\
	\lambda_5 &= \begin{pmatrix} 0 & 0 & -i \\ 0 & 0 & 0 \\ i & 0 & 0 \end{pmatrix},
	\lambda_6 = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix},
	\lambda_7 = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & -i \\ 0 & i & 0 \end{pmatrix},
	\lambda_8 = \frac{1}{\sqrt{3}} \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -2 \end{pmatrix},
\end{align}
giving rise to 8 gauge bosons of the strong interaction, known as gluons, each having a property called color. The reason for having 8 gluons is that we need 8 matrices to span the 'color' space laid out by the Gell-Mann matrices, while requiring that they have determinant 1. This is in fact the same reason why we need three Pauli matrices to span the group of SU(2). Because of this color feature, the gauge theory of the strong interaction is called Quantum Chromodynamics (QCD).

\subsection{The Standard Model}
Combining electroweak theory and QCD, using the group U(1) $\times$ SU(2) $\times$ SU(3), physicists are able to account for a great variety of elementary phenomena, particles and interactions, and make profoundly accurate predictions. This theory, is know as the Standard Model of particle physics. The Standard Model is a non abelian gauge theory.\footnote{Non abelian means that, save for U(1), the generators of the gauge groups are non-commuting.}

\subsubsection{Particles of the standard model} \label{sec:particles}

% Billede med partikel-generationer fra Wikipedia.
\includefigure{fig:particle_generations}{0.3}{./images/particle_generations}{The fundamental particles in the Standard Model. Fermions and bosons, listed in their respective generations. (Source: Wikimedia Commons).}

In the Standard Model, we divide particles into two main groups; fermions and bosons. Fermions are defined as having half-integral spin\footnote{Spin is a fundamental quantum mechanical intrinsic degree of freedom. We do not, as with many other intrinsic properties, have a macroscopic analog of spin.} and are described by Fermi-Dirac statistics, these include the leptons and quarks. All fermions have electric charge except for the neutrinos, all quarks have an extra property called color charge, which makes them interact with gluons. Fermions constitutes all known matter and as such they are sometimes described as matter-particles. Bosons, on the other hand, have zero or integral spin and are described by Bose-Einstein statistics. Some of these, namely the gauge bosons, are responsible for the weak, strong and electromagnetic interactions. Because of this, the gauge bosons are often called the force-carriers of their respective interactions. It may be appropriate to note that the formalism of quantum mechanics makes no clear distinction between the concepts of matter-particles and force-particles. In Figure \ref{fig:particle_generations} is a table with all known quarks, leptons and gauge bosons. With the exception of neutrinos, which are very difficult to detect, all of them have been seen, directly or indirectly, in particle accelerator experiments.

\subsubsection{Feynman diagrams}
When calculating amplitudes for particle interactions in the SM, it can be very instructive to make use of \emph{Feynman diagrams}. This methodology first proposed by American physicist Richard P. Feynman in 1949 \cite{feynman1949sta}. A typical space-time interaction is the scattering of an electron and a positron by virtual annihilation followed by pair production. This process is represented as a Feynman diagram in Figure \ref{fig:feyn:ee_a_ee}. Space is extending top-down in the diagram while time is increasing from left to right. Arrows pointing along the flow of time, to the right, are particles, while arrows pointing against the flow of time, to the left, are anti-particles.
\begin{figure}[htp]
\centering
	\input{./jaxo/ee_a_ee_axis.tex}
\caption{Feynman diagram for the electromagnetic scattering of an electron and a positron. Time is increasing from left to right in the diagram.} \label{fig:feyn:ee_a_ee}
\end{figure}
The wavy line in the middle is a so called \emph{virtual particle}. In this case a virtual photon. It is virtual in the sense that we never actually measure it, without disturbing the process and thereby destroying it. We can only ever measure the initial-state (incoming) particles and the final-state (outgoing) particles. Another reason why these, intermediate particles, are called virtual is seen by looking at Figure \ref{fig:feyn:ee_a}. In the centre-of-mass (CM) frame of the electron and positron, the two have exactly equal and opposite momenta. Together, they have zero momentum in the CM-frame. Therefore, the two particles can never annihilate to create a single massless photon, which will always have momentum, given that it always travels at the speed $c$. \cite{griffiths1987iep}
\begin{figure}[htp]
\centering
	\input{./jaxo/ee_a.tex}
\caption{Example of a virtual process in which an electron and a positron annihilates to create a virtual photon, seemingly violating conservation of momentum at the vertex. This process can never happen on its own, but it can form part of other diagrams, i.e. like that in Figure \ref{fig:feyn:ee_a_ee}.} \label{fig:feyn:ee_a}
\end{figure}

To uphold energy and momentum conservation at the vertex, the virtual photon have to have an energy different from that of a free photon. In principle, it can take on \emph{any} value, but it is most likely to choose one that lies close to the value of the free photon. The virtual photon is said to be \emph{off mass shell} or simply \emph{off-shell}. One explanation for why this can happen can be found in Heisenberg's uncertainty relation \eqref{eq:heisenberg}.

\subsubsection{Feynman rules and amplitudes}
Using these diagrams, it is possible to greatly simplify how we think about, and calculate, particle interactions in the SM. Having drawn a diagram as in Figure \ref{fig:feyn:ee_a_ee}, we want to calculate the probability for this process to happen. That is to say, we want the \emph{cross section} $\sigma$ of the process.\footnote{See section \ref{sec:lhc} about collider experiments for a discussion of cross sections.} Given the diagram one can write down the \emph{amplitude}, $\mathcal{M}$, for this process, using so called \emph{Feynman rules} associated with the different parts of the diagram. Given $\mathcal{M}$ we can calculate the cross section using Fermi's Golden Rule \eqref{eq:goldenrule}. The amplitude, or the \emph{matrix element} as it is some times called, accounts for the dynamics taking place in the interaction, while the Golden Rule, or \emph{phase space} factor, applies the kinematics.

To derive the actual feynman rules is beyond the scope of this paper, but we may still learn a lot by examining the results. Each vertex represents an interaction. Depending on the nature of this interaction we assign a number $g$ related to the coupling between the lines, or particles, going into the vertex. That number is proportional to the probability for the process to happen. For the diagram in Figure \ref{fig:feyn:ee_a_ee} the interaction at play in the vertices is the electromagnetic force. The strength of this interaction is determined by the fine structure constant

\begin{equation}
	\alpha = 4\pi g_e^2 = \frac{e^2}{4\pi\varepsilon_0\hbar c},
\end{equation}

where $e$ is the charge of the positron, $\varepsilon_0$ is the permittivity of free space, $c$ is the speed of light in vacuum and $\hbar$ is the reduced Planck constant. In natural units, setting $c = 1$ and $\hbar = 1$, and at low energies, $\alpha$ is approximately equal to $1/137$. Constants like these are called coupling constants and they are always dimensionless.\footnote{At increasing energies, these constants are no longer constant, but are observed to be running. Therefore, they are also called running coupling constants. At energies of around $10^{14}$ GeV three of the fundamental forces (all except gravity) are thought to combine into one grand unified force with one coupling strength $\alpha_{\textrm{\tiny{GUT}}}$. GUT stands for \emph{Grand Unified Theory}.}

The other parts of the diagram get their own factors according to the feynman rules. Each external, incoming or outgoing, line get a factor related to the type of particle and its direction, i.e. if its a particle or an antiparticle. Internal lines, also called propagators, get different factors because they don't lie on their mass shell and are what we call virtual particles. We then use delta functions to impose energy momentum conservation at the vertices and finally we integrate over internal momenta\footnote{Remember we can never measure the virtual particles, so we have to take into account all the different states the propagator can have been in. That is all the different momenta it could have had, as they all contribute to the amplitude. This is closely related to the Feynman propagator discussed in section \ref{sec:feynpath}.}. Following the above rules, one can write down the amplitude, or matrix element, $\mathcal{M}$. Having written down the amplitude for the process, the next step is to calculate the actual cross section for the process. To do this we use Fermi's Golden Rule.
% In case of Figure \ref{fig:feyn:ee_a_ee} the matrix element is given by \cite{griffiths1987iep}
% 
% \begin{equation}
% 	\mathcal{M} = -\frac{g_e^2}{(p_1 + p_2)^2} (\bar u(3) \gamma^\mu v(4)) (\bar v(2) \gamma_\mu u(1)),
% \end{equation}
% where [SKRIV HVAD u OG v ER!].
\subsubsection{Fermi's golden rule for scattering}
Fermi's golden rule states that, for scattering between incoming particles with 4-momenta $p_1, p_2$ producing $n$ particles with momenta $p_3 \dots , p_n$, the scattering cross section is given by \cite{griffiths1987iep}
\begin{align} \label{eq:goldenrule}
	\hat \sigma = &\frac{S\hbar^2}{4\sqrt{(p_1 \cdot p_2)^2 - (m_1 m_2 c^2)^2}} \int |\mathcal{M}|^2 (2 \pi)^4 \delta^4(p_1+p_2-p_3 \dots -p_n) \nonumber \\
	&\times \prod_{j=3}^n 2\pi \delta(p_j^2 - m_j^2c^2)\theta(p_j^0) \frac{\textrm{d}p_j}{(2\pi)^2}.
\end{align}
Here $S$ is a factor taking care of double counting of identical particles in the final state and $\theta$ is the Heaviside step function defined by
\begin{equation}
	 \theta(x) = 
	\begin{cases} 
	  0,  & \mbox{for }x < 0 \\
	  1,  & \mbox{for }x > 0 
	\end{cases}.
\end{equation}
Equation \eqref{eq:goldenrule} may look confusing at first, but all it is saying is, that momentum and energy has to be conserved, going from initial to final state, and outgoing energy must be positive. This is what the delta function and the step function takes care of. The rest is basically integration over outgoing momenta. All the dynamics, is hidden inside the matrix element, $\mathcal{M}$, derived from the feynman rules of the theory.

In the special case, where we have only two particles in the final state, and we look at the CM frame,\footnote{In this case $\sqrt{(p_1\cdot p_2)^2 - (m_1m_2 c^2)^2} = (E_1+E_2)|\mathbf{p}_1|/c$. See section 6.2 in \cite{griffiths1987iep}.} equation \eqref{eq:goldenrule} reduces to \cite{griffiths1987iep}

\begin{equation}
	\hat \sigma = \frac{S\hbar^2c}{64\pi^2(E_1+E_2)|\mathbf{p}_1|} \int | \mathcal{M} |^2
	\frac{\delta^4(p_1+p_2-p_3-p_4)}{\sqrt{\mathbf{p}_3^2+m_3^2c^2}\sqrt{\mathbf{p}_4^2+m_4^2c^2}} \textrm{d}^3\mathbf{p}_3\textrm{d}^3\mathbf{p}_4.
\end{equation}

The previous discussion illustrated, that using very simple arguments, like local gauge invariance, we can, when plugging in the particle wave functions and their masses, derive an amazing amount of physical results. Results that can be tested almost directly in experiments like the LHC at CERN.

\subsubsection{Perturbation theory and renormalization}
So far everything seems fine. But as usual, Nature has a few surprises up its sleeve. The diagram in Figure \ref{fig:feyn:ee_a_ee} is not the only way this process can happen. To understand why this is the case, take a look at Figure \ref{fig:feyn:ee_a_ee_2}. In this diagram, the virtual photon splits up into a electron/positron pair which again annihilates into a photon. This process if perfectly legal according to everything we know from the previous discussions, and as Feynman said; everything that \emph{can} happen \emph{will} happen. The problem arises because we can't decide which one happened, without disturbing the process all together. Therefore,  we have to take them both into consideration. To calculate the total amplitude for the process we have to include diagrams of these higher order perturbations as well. In fact we can add as many electron/positron pairs as we like. In principle, if we want to know the exact probability, we would have to include infinitely many diagrams in our calculation. So much for amazing predictive power.

What saves us is the fact that, as we add more vertices, the overall probability is multiplied by factors of $g_e^2 \ll 1$ for each vertex, so the total amplitude converges. Disaster is avoided by the use of perturbation theory.
\begin{figure}[htp]
\centering
	\input{./jaxo/ee_a_ee_2.tex}
\caption{Higher order contribution to the process of Figure \ref{fig:feyn:ee_a_ee}. The loop in the middle represents production of a virtual electron/positron pair which, again, annihilates to produce a photon, giving rise to a net polarization of the vacuum. It is impossible to make an experiment which distinguishes between this diagram and the tree-level diagram in Figure \ref{fig:feyn:ee_a_ee}.} \label{fig:feyn:ee_a_ee_2}
\end{figure}
Loop type diagrams, as the one in Figure \ref{fig:feyn:ee_a_ee_2}, poses another, more fundamental, problem. Just as we had to sum over all possible diagrams, with equal initial and final states, we have to integrate over all possible momenta $p$ of the virtual electron/positron pair in the loop. This integral turns out to depend mainly on $1 / p^4$ and the volume element $p^3$, so we end up with an integral of the form\footnote{See section 6.3 in \cite{griffiths1987iep}.}
\begin{equation}
	\int^\infty \frac{1}{p^4}p^3 dp = \ln{p}|^\infty = \infty,
\end{equation}
which evidently diverges logarithmically. Again we are saved, but this time the argument is a lot more subtle, and took physicists many years to develop. The infinities are removed by the technique of renormalization. Essentially, what we do is that we normalize by an infinite normalization constant, effectively and exactly canceling the divergent loop integrals. This is somewhat an empirical motivation, as we know that the probability has to be finite (actually is has to be $\leq 1$), but also because we know that the energies of the free particles have to be finite. One way to think of it is that we include all loop type vacuum oscillations in the description of the particle. Since we can only ever measure the total energy, including vacuum oscillations in the immediate vicinity of the particle, we simply \emph{renormalize} the otherwise infinite result when calculating the particle's energy, to arrive at the measured value. This not only goes for the masses $m$ of particles, but the couplings $g$ as well \cite{griffiths1987iep}
\begin{align}
	m_{\textrm{physical}} &= m + \delta m \nonumber \\
	g_{\textrm{physical}} &= g + \delta g.
\end{align}
What we're saying is that, although $\partial m$ and $\partial g$ are infinite, we can ever only measure the physical values. So we don't have to care what $m$ and $\partial m$ are, simply that they somehow cancel each other out to give a finite result. The complete discussion of renormalization is naturally a bit more complicated. For an account on how to do the above procedure see \cite{sakurai1967aqm}. Renormalizability is thus a fundamental criteria that any gauge theory hoping to make sensible predictions about the physical observable world has to satisfy. We will come back to renormalization when talking about general relativity and quantum gravity, especially because this is a prime motivator in the development of noncommutative geometry.
