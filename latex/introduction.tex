\section{Introduction (In English)}
In describing fundamental physical phenomena, physicists make use of the concept of a particle representing a certain small-scale state (of the order $10^{-15}$ m) of the universe. Particles can interact with each other annihilating or creating new particles (or states). To account for this we use another concept, namely that of interactions. With these two concepts in hand we can go on and build mathematical frameworks based on experimental results and/or purely mathematical ideas, with the intend to extend our knowledge of the Universe and increase the predictive power and accuracy of the theories involved. Using this method physicists have been able to create an extensive theoretical framework of amazing predictive accuracy. This framework, known as the Standard Model (SM) of particle physics, is written in the mathematical language of Quantum Field Theory (QFT). In the context of QFT particles are thought of as excitations in underlying fields propagating through space-time.

Another accurate framework was developed in the beginning of the last century by german physicist Albert Einstein. This framework, describing gravity on large scales, is know as the General Theory of Relativity (GR). Together with the SM these two theories represent our best current knowledge of the physical Universe at the very large and very small scales.\footnote{Henceforth when we write knowledge the reader may assume this is equivalent to the information available from being able to predict the time evolution of physical systems or states.}

These two frameworks, although very useful in their own domains, are mutually incompatible. The problem arises in the definition of the fabric on wich events, or processes occur. This fabric is made up of the 3 partial dimensions and 1 time-like dimension together forming a 4-dimensional manifold called spacetime.

In GR spacetime is thought of as a grid with infinitely many points, whilst in QFT (and therefore the SM) spacetime is described as a multitude of fluctuating fields resulting in a net polarization of the vacuum. In this realm points in space are no longer well defined as otherwise required by GR.

A lot of effort has been put in to unifying the encompassing physical theory of the Universe on the very largest scales GR, with the theory of Universe at the smallest scales, the SM. So far no one have been successful, but a lot of interesting theoretical candidates exists. In the following report we will concentrate on one of these candidates, namely the extension of the SM by Non-Commutative Geometry (NCG). We will see that this theory predicts new vertices between particles that are illegal in the normal context of the SM. These vertices arise from interactions between the neutral gauge boson Z$^0$, responsible for weak-force decays, and the charged gauge bosons the gluons ($g$), responsible for strong-force decays.

In the report we will also analyse the particular conditions necessary for the interactions to be observed in experiments such as LEP and LHC. Furthermore will explore the different constraints that current experimental data puts on the detailed structure of this particular NCG-extension of the SM.


\section{Introduction (In Danish)}


