\section{Introduction In English}
In describing fundamental physical phenomena, physicists make use of the concept of a particle representing a certain small-scale state (of the order $10^{-15}$ m) of the universe. Particles can interact with each other annihilating or creating new particles (or states). To account for these phenomena we use make use of the notion of interaction between the particles. In particle physics the description of an interaction accounts for the particles involved, their intrinsic characteristics, conversed quantities, information about the before/ after state, and so on.

With these two concepts at hand we can go on and build mathematical frameworks based on experimental results and/or purely mathematical ideas, with the intend to extend our knowledge of the Universe and increase the predictive power and accuracy of the theories involved. Using this method physicists have been able to create an extensive theoretical framework of amazing predictive accuracy. This framework, known as the Standard Model (SM) of particle physics, is written in the mathematical language of Quantum Field Theory (QFT). In the context of QFT particles are thought of as excitations in underlying fields propagating through spacetime.

Another accurate framework was developed in the beginning of the last century by german physicist Albert Einstein. This framework, describing gravity on large scales, is know as the General Theory of Relativity (GR). Together with the SM these two theories represent our best current knowledge of the physical Universe at the very large and very small scales.\footnote{Henceforth when we write knowledge the reader may assume this is equivalent to the information available from being able to predict the time evolution of physical systems or states.}

These two frameworks, although very useful in their own domains, are mutually incompatible. In GR spacetime is thought of as a smooth and continuous manifold, whilst in QFT spacetime, on the smallest scales, is a turmoil of fluctuating fields. From a macroscopic perspective this might not seem like a problem, after all the fluctuations are tiny, but when we try to calculate the effects of gravity at these scale we and up with horribly divergent results, i.e. infinities. The problem is that when we try to include gravity in the quantum mechanical description (quantum gravity) we end up with a theory that is non-renormalizable.

A lot of effort has been put in to unifying the encompassing physical theory of the Universe on the very largest scales, GR, with the theory of Universe at the smallest scales, the SM. So far no one have been successful, but a lot of interesting theoretical candidates exists.

In the following report we will concentrate on one of these candidates, namely the extension of the SM by Non-Commutative Geometry (NCG). We will see that this theory predicts new vertices between particles that are illegal in the normal context of the SM. These vertices arise from interactions between the neutral gauge boson Z$^0$, responsible for weak-force decays, and the charged gauge bosons the gluons ($g$), responsible for strong-force decays.

We will also analyse the particular conditions necessary for the interactions to be observed in experiments such as LEP and LHC. Furthermore will explore the different constraints that current experimental data puts on the detailed structure of this particular NCG-extension of the SM.

\newpage
\section{Introduction In Danish}